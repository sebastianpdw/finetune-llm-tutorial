{
  "run_name": "meta-llama-3-1-8b-instruct-4bit",
  "hyperstack_deployment": {
    "enabled": true,
    "environment_name": "default-NORWAY-1",
    "vm_name": "finetuning-norway-vm",
    "flavor_name": "n3-RTX-A4000x1",
    "image_name": "Ubuntu 22.04 LTS Server R535 CUDA 12.2 with Docker",
    "delete_after_completion": false
  },
  "finetuning": {
    "enabled": true,
    "use_host_hf_token": true,
    "model_name_or_path": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "load_model_in_4bit": true,
    "dataset_csv_path": "data/datasets/train.csv",
    "num_train_epochs": 10,
    "peft": "lora",
    "peft_hyperparams": {
      "r": 8,
      "lora_alpha": 32,
      "lora_dropout": 0.2
    }
  },
  "inference": {
    "enabled": true,
    "mode": "csv",
    "mode_configs": {
      "csv": {
        "input_path": "data/datasets/test.csv",
        "input_column": "user",
        "output_column": "response"
      }
    },
    "model_hyperparams": {
      "temperature": 0.5,
      "max_length": 500
    }
  },
  "app_deployment": {
    "enabled": true,
    "use_host_hf_token": true,
    "model_run_dir": "latest",
    "base_model": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "additional_vllm_args": [
      "--max-model-len 4096",
      "--gpu_memory_utilization 0.6",
      "--quantization bitsandbytes",
      "--load-format bitsandbytes"
    ]
  }
}
